{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS SPAM/HAM CLASSIFIER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import *\n",
    "from sklearn.dummy import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.calibration import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(3)\n",
    "pd.set_option('display.max_columns', None) #to view all the columns data from a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to analyse classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_models(classifiers, vectorizers, train_data, test_data):\n",
    "    \"\"\"\n",
    "    This function analyses the performance of given combination of classifier and vectorizer\n",
    "    Input:\n",
    "        classifiers - list of classifiers used for analysis\n",
    "        vectorizers - list of vectorizers used for analysis\n",
    "        train_data - training data\n",
    "        test_data - Testing data\n",
    "        \n",
    "    Output:\n",
    "        results - contains the classifier, vectorizer along with their scores on test and train data\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for classifier in classifiers:\n",
    "      for vectorizer in vectorizers:\n",
    "        string = ''\n",
    "        string += classifier.__class__.__name__ + ' with ' + vectorizer.__class__.__name__\n",
    "\n",
    "        # train\n",
    "        vectorize_text = vectorizer.fit(train_data.features)\n",
    "        \n",
    "        train_vectorized_text = vectorize_text.transform(train_data.features)\n",
    "        \n",
    "        trained_classifier = classifier.fit(train_vectorized_text, train_data.label)\n",
    "        \n",
    "        train_score = trained_classifier.score(train_vectorized_text, train_data.label)\n",
    "        train_f1_score = f1_score(trained_classifier.predict(train_vectorized_text), train_data.label)\n",
    "        \n",
    "\n",
    "        # test\n",
    "        test_vectorized_text = vectorize_text.transform(test_data.features)\n",
    "        \n",
    "        test_score = trained_classifier.score(test_vectorized_text, test_data.label)\n",
    "        test_f1_score = f1_score(trained_classifier.predict(test_vectorized_text), test_data.label)\n",
    "        \n",
    "        string += ' Has Scores: ' + ' Train : ' + str(train_score) + ' Test : ' + str(test_score) \\\n",
    "                    + ' Train : ' + ' and F1 Scores : ' + ' Train : ' + str(train_f1_score) + ' Test : ' \\\n",
    "                    + str(test_f1_score) + '\\n'\n",
    "        \n",
    "        results.append([classifier, vectorizer, str(train_score), str(test_score), str(train_f1_score), \n",
    "                       str(test_f1_score)])\n",
    "        print(string)\n",
    "        \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    \"\"\"\n",
    "    This function preprocesses the given data\n",
    "    \n",
    "    preprocess - \n",
    "                    converting the data to lower case\n",
    "                    removing special characters other than dollar($)\n",
    "                    to represent amounts as different categories\n",
    "    Input:\n",
    "        data - the data to be preprocessed\n",
    "    \n",
    "    Output:\n",
    "        numpy array containing the preprocessed data\n",
    "    \n",
    "    \"\"\"\n",
    "    preprocessed_data = data.str.lower()\n",
    "    preprocessed_data = preprocessed_data.map(lambda document: re.split('[^a-zA-Z0-9\\'$]', document))\n",
    "    preprocessed_data = preprocessed_data.map(lambda document: list(filter(lambda word : len(word)>1, document)))\n",
    "    preprocessed_data = preprocessed_data.map(lambda document: ' '.join(document))\n",
    "    \n",
    "    return preprocessed_data.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data, train = True, tfidf_vectorizer = None):\n",
    "    \"\"\"\n",
    "    This function fits the vectorizer with train data and transforms the test data using given vectorizer\n",
    "    \n",
    "    Input:\n",
    "        data - data to be used to fit the vectorizer or to be transformed\n",
    "    \n",
    "    Output:\n",
    "        A python dictionary with the transformed feature data as 'features' and fitted vectorizer as 'vectorizer'\n",
    "    \n",
    "    \n",
    "    If train = True : \n",
    "        fits the vectorizer with train data\n",
    "    else : \n",
    "        transforms the test data using given vectorizer\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        \n",
    "        #initialising Vectorizer\n",
    "        tfidf = TfidfVectorizer()\n",
    "        \n",
    "        #Fitting the vectorizers with train data\n",
    "        tfidf_vectorizer = tfidf.fit(data)\n",
    "        tfidf_vetorized_text = tfidf_vectorizer.transform(data)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #Transforming the Test data\n",
    "        tfidf_vetorized_text = tfidf_vectorizer.transform(data)\n",
    "        \n",
    "    return {'features' : tfidf_vetorized_text, \n",
    "            'tfidf_vectorizer' : tfidf_vectorizer}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to train the stacking classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(features, label, stacking_models, cv = 10):\n",
    "    \"\"\"\n",
    "    This function trains the stacking classifier with given estimators, data and returns the trained estimator \n",
    "    \n",
    "    Input:\n",
    "        features - feature data for the model\n",
    "        label - label data for the model\n",
    "        stacking_models - estimators to be used for stacking\n",
    "        cv - cross validation for stacking classifier\n",
    "        \n",
    "    Ouput:\n",
    "        A python Dictionary containing the trained model as 'model' and the stacking classifier as 'classifier'\n",
    "    \"\"\"\n",
    "    st_classifier = StackingClassifier( estimators = stacking_models, final_estimator = SVC(kernel='rbf', random_state = 3), \n",
    "                                   n_jobs = -1, cv = cv, stack_method = 'predict')\n",
    "    print(\"Trainging with \", list(map(lambda x:x[0], stacking_models)), sep = '\\n')\n",
    "    trained_model = st_classifier.fit(features, label)\n",
    "    print(\"Training Score : \", trained_model.score(features, label))\n",
    "    return {'model' : trained_model, 'classifier': st_classifier}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the SMS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>RESULT</th>\n",
       "      <th>SMS</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   No RESULT                                                SMS  label\n",
       "0   1    ham  Go until jurong point, crazy.. Available only ...    0.0\n",
       "1   2    ham                      Ok lar... Joking wif u oni...    0.0\n",
       "2   3   spam  Free entry in 2 a wkly comp to win FA Cup fina...    1.0\n",
       "3   4    ham  U dun say so early hor... U c already then say...    0.0\n",
       "4   5    ham  Nah I don't think he goes to usf, he lives aro...    0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_data = pd.read_csv(r\"D:\\Narendra\\AIGenie_Capstone_ALL\\sms.csv\", encoding = 'latin1') # Update the file path\n",
    "sms_data['label'] = sms_data['RESULT'].replace({'ham':0, 'spam':1}) # Converting the result feature into integer labels\n",
    "sms_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_data.columns = ['No', 'Result', 'sms', 'label'] # Renaming the feature names of SMS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating the train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the data contains both train and test data in the same file. So, we need to separate the data into train and test data.\n",
    "Data with label is used for training and without label is used for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering data from pandas dataframe\n",
    "test_data = sms_data[sms_data.isna().sum(axis = 1) > 0] \n",
    "train_data = sms_data[sms_data.isna().sum(axis = 1) == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5540, 5), (32, 5))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['features'] = preprocess(train_data.loc[:, 'sms'])\n",
    "test_data['features'] = preprocess(test_data.loc[:, 'sms'])\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Train data into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x = train_test_split(train_data[['features', 'label']], test_size = 0.2, \n",
    "                                    shuffle = True, stratify = train_data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the Classifier and Vectorizer models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB with CountVectorizer Has Scores:  Train : 0.986687725631769 Test : 0.9828519855595668 Train :  and F1 Scores :  Train : 0.9478337754199824 Test : 0.9314079422382672\n",
      "\n",
      "BernoulliNB with TfidfVectorizer Has Scores:  Train : 0.986687725631769 Test : 0.9828519855595668 Train :  and F1 Scores :  Train : 0.9478337754199824 Test : 0.9314079422382672\n",
      "\n",
      "BernoulliNB with HashingVectorizer Has Scores:  Train : 0.8664259927797834 Test : 0.8664259927797834 Train :  and F1 Scores :  Train : 0.0 Test : 0.0\n",
      "\n",
      "RandomForestClassifier with CountVectorizer Has Scores:  Train : 1.0 Test : 0.9846570397111913 Train :  and F1 Scores :  Train : 1.0 Test : 0.9390681003584229\n",
      "\n",
      "RandomForestClassifier with TfidfVectorizer Has Scores:  Train : 1.0 Test : 0.9846570397111913 Train :  and F1 Scores :  Train : 1.0 Test : 0.9390681003584229\n",
      "\n",
      "RandomForestClassifier with HashingVectorizer Has Scores:  Train : 1.0 Test : 0.9774368231046932 Train :  and F1 Scores :  Train : 1.0 Test : 0.9077490774907748\n",
      "\n",
      "AdaBoostClassifier with CountVectorizer Has Scores:  Train : 0.9799187725631769 Test : 0.973826714801444 Train :  and F1 Scores :  Train : 0.9208888888888889 Test : 0.8982456140350877\n",
      "\n",
      "AdaBoostClassifier with TfidfVectorizer Has Scores:  Train : 0.9846570397111913 Test : 0.9720216606498195 Train :  and F1 Scores :  Train : 0.9404553415061296 Test : 0.8904593639575972\n",
      "\n",
      "AdaBoostClassifier with HashingVectorizer Has Scores:  Train : 0.9803700361010831 Test : 0.9711191335740073 Train :  and F1 Scores :  Train : 0.9223907225691348 Test : 0.8857142857142858\n",
      "\n",
      "BaggingClassifier with CountVectorizer Has Scores:  Train : 0.9963898916967509 Test : 0.9774368231046932 Train :  and F1 Scores :  Train : 0.9863247863247863 Test : 0.9122807017543859\n",
      "\n",
      "BaggingClassifier with TfidfVectorizer Has Scores:  Train : 0.9972924187725631 Test : 0.9729241877256317 Train :  and F1 Scores :  Train : 0.9897785349233391 Test : 0.8943661971830986\n",
      "\n",
      "BaggingClassifier with HashingVectorizer Has Scores:  Train : 0.9970667870036101 Test : 0.9783393501805054 Train :  and F1 Scores :  Train : 0.988936170212766 Test : 0.916083916083916\n",
      "\n",
      "ExtraTreesClassifier with CountVectorizer Has Scores:  Train : 1.0 Test : 0.9873646209386282 Train :  and F1 Scores :  Train : 1.0 Test : 0.950354609929078\n",
      "\n",
      "ExtraTreesClassifier with TfidfVectorizer Has Scores:  Train : 1.0 Test : 0.9873646209386282 Train :  and F1 Scores :  Train : 1.0 Test : 0.950354609929078\n",
      "\n",
      "ExtraTreesClassifier with HashingVectorizer Has Scores:  Train : 1.0 Test : 0.98014440433213 Train :  and F1 Scores :  Train : 1.0 Test : 0.9197080291970803\n",
      "\n",
      "GradientBoostingClassifier with CountVectorizer Has Scores:  Train : 0.9817238267148014 Test : 0.9747292418772563 Train :  and F1 Scores :  Train : 0.9277430865298839 Test : 0.8970588235294118\n",
      "\n",
      "GradientBoostingClassifier with TfidfVectorizer Has Scores:  Train : 0.9862364620938628 Test : 0.973826714801444 Train :  and F1 Scores :  Train : 0.9460654288240495 Test : 0.8937728937728938\n",
      "\n",
      "GradientBoostingClassifier with HashingVectorizer Has Scores:  Train : 0.9848826714801444 Test : 0.9720216606498195 Train :  and F1 Scores :  Train : 0.9407603890362513 Test : 0.8880866425992779\n",
      "\n",
      "DecisionTreeClassifier with CountVectorizer Has Scores:  Train : 1.0 Test : 0.9657039711191335 Train :  and F1 Scores :  Train : 1.0 Test : 0.87248322147651\n",
      "\n",
      "DecisionTreeClassifier with TfidfVectorizer Has Scores:  Train : 1.0 Test : 0.9657039711191335 Train :  and F1 Scores :  Train : 1.0 Test : 0.8642857142857143\n",
      "\n",
      "DecisionTreeClassifier with HashingVectorizer Has Scores:  Train : 1.0 Test : 0.9720216606498195 Train :  and F1 Scores :  Train : 1.0 Test : 0.8934707903780069\n",
      "\n",
      "CalibratedClassifierCV with CountVectorizer Has Scores:  Train : 1.0 Test : 0.9882671480144405 Train :  and F1 Scores :  Train : 1.0 Test : 0.9547038327526132\n",
      "\n",
      "CalibratedClassifierCV with TfidfVectorizer Has Scores:  Train : 0.9995487364620939 Test : 0.990072202166065 Train :  and F1 Scores :  Train : 0.9983136593591906 Test : 0.9624573378839592\n",
      "\n",
      "CalibratedClassifierCV with HashingVectorizer Has Scores:  Train : 0.9984205776173285 Test : 0.9873646209386282 Train :  and F1 Scores :  Train : 0.9940728196443691 Test : 0.952054794520548\n",
      "\n",
      "PassiveAggressiveClassifier with CountVectorizer Has Scores:  Train : 1.0 Test : 0.9864620938628159 Train :  and F1 Scores :  Train : 1.0 Test : 0.9477351916376306\n",
      "\n",
      "PassiveAggressiveClassifier with TfidfVectorizer Has Scores:  Train : 1.0 Test : 0.990072202166065 Train :  and F1 Scores :  Train : 1.0 Test : 0.9624573378839592\n",
      "\n",
      "PassiveAggressiveClassifier with HashingVectorizer Has Scores:  Train : 1.0 Test : 0.9864620938628159 Train :  and F1 Scores :  Train : 1.0 Test : 0.9477351916376306\n",
      "\n",
      "RidgeClassifier with CountVectorizer Has Scores:  Train : 1.0 Test : 0.983754512635379 Train :  and F1 Scores :  Train : 1.0 Test : 0.935251798561151\n",
      "\n",
      "RidgeClassifier with TfidfVectorizer Has Scores:  Train : 0.9972924187725631 Test : 0.9864620938628159 Train :  and F1 Scores :  Train : 0.9897610921501706 Test : 0.9477351916376306\n",
      "\n",
      "RidgeClassifier with HashingVectorizer Has Scores:  Train : 0.9943592057761733 Test : 0.983754512635379 Train :  and F1 Scores :  Train : 0.9784296807592753 Test : 0.9366197183098591\n",
      "\n",
      "RidgeClassifierCV with CountVectorizer Has Scores:  Train : 1.0 Test : 0.983754512635379 Train :  and F1 Scores :  Train : 1.0 Test : 0.935251798561151\n",
      "\n",
      "RidgeClassifierCV with TfidfVectorizer Has Scores:  Train : 1.0 Test : 0.9864620938628159 Train :  and F1 Scores :  Train : 1.0 Test : 0.9477351916376306\n",
      "\n",
      "RidgeClassifierCV with HashingVectorizer Has Scores:  Train : 0.9997743682310469 Test : 0.9855595667870036 Train :  and F1 Scores :  Train : 0.9991546914623838 Test : 0.943661971830986\n",
      "\n",
      "SGDClassifier with CountVectorizer Has Scores:  Train : 0.9995487364620939 Test : 0.9819494584837545 Train :  and F1 Scores :  Train : 0.9983079526226735 Test : 0.9290780141843972\n",
      "\n",
      "SGDClassifier with TfidfVectorizer Has Scores:  Train : 1.0 Test : 0.990072202166065 Train :  and F1 Scores :  Train : 1.0 Test : 0.9619377162629756\n",
      "\n",
      "SGDClassifier with HashingVectorizer Has Scores:  Train : 1.0 Test : 0.983754512635379 Train :  and F1 Scores :  Train : 1.0 Test : 0.937062937062937\n",
      "\n",
      "OneVsRestClassifier with CountVectorizer Has Scores:  Train : 0.9997743682310469 Test : 0.9882671480144405 Train :  and F1 Scores :  Train : 0.9991546914623838 Test : 0.9543859649122808\n",
      "\n",
      "OneVsRestClassifier with TfidfVectorizer Has Scores:  Train : 0.9963898916967509 Test : 0.990072202166065 Train :  and F1 Scores :  Train : 0.9863247863247863 Test : 0.9621993127147767\n",
      "\n",
      "OneVsRestClassifier with HashingVectorizer Has Scores:  Train : 0.993456678700361 Test : 0.9864620938628159 Train :  and F1 Scores :  Train : 0.9749784296807593 Test : 0.9473684210526316\n",
      "\n",
      "OneVsRestClassifier with CountVectorizer Has Scores:  Train : 0.9975180505415162 Test : 0.9855595667870036 Train :  and F1 Scores :  Train : 0.9906223358908782 Test : 0.943661971830986\n",
      "\n",
      "OneVsRestClassifier with TfidfVectorizer Has Scores:  Train : 0.9749548736462094 Test : 0.9747292418772563 Train :  and F1 Scores :  Train : 0.8969359331476323 Test : 0.8955223880597014\n",
      "\n",
      "OneVsRestClassifier with HashingVectorizer Has Scores:  Train : 0.9758574007220217 Test : 0.9720216606498195 Train :  and F1 Scores :  Train : 0.9021043000914915 Test : 0.8838951310861423\n",
      "\n",
      "KNeighborsClassifier with CountVectorizer Has Scores:  Train : 0.930956678700361 Test : 0.9232851985559567 Train :  and F1 Scores :  Train : 0.6514806378132119 Test : 0.5971563981042654\n",
      "\n",
      "KNeighborsClassifier with TfidfVectorizer Has Scores:  Train : 0.9189981949458483 Test : 0.9151624548736462 Train :  and F1 Scores :  Train : 0.5648484848484848 Test : 0.5346534653465346\n",
      "\n",
      "KNeighborsClassifier with HashingVectorizer Has Scores:  Train : 0.9305054151624549 Test : 0.9250902527075813 Train :  and F1 Scores :  Train : 0.65 Test : 0.6103286384976526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models_summary = analyze_models(\n",
    "    [\n",
    "        BernoulliNB(),\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state = 3),\n",
    "        AdaBoostClassifier(random_state = 3),\n",
    "        BaggingClassifier(random_state = 3, n_jobs = -1),\n",
    "        ExtraTreesClassifier(random_state = 3, n_jobs = -1),\n",
    "        GradientBoostingClassifier(random_state = 3),\n",
    "        DecisionTreeClassifier(random_state = 3),\n",
    "        CalibratedClassifierCV(),\n",
    "        PassiveAggressiveClassifier(random_state = 3, n_jobs = -1),\n",
    "        RidgeClassifier(random_state = 3),\n",
    "        RidgeClassifierCV(),\n",
    "        SGDClassifier(loss = 'modified_huber', random_state = 3, n_jobs = -1),\n",
    "        OneVsRestClassifier(SVC(kernel='linear', random_state = 3, probability = True)),\n",
    "        OneVsRestClassifier(LogisticRegression(random_state = 3, n_jobs = -1)),\n",
    "        KNeighborsClassifier()\n",
    "    ],\n",
    "    [\n",
    "        CountVectorizer(),\n",
    "        TfidfVectorizer(),\n",
    "        HashingVectorizer()\n",
    "    ],\n",
    "    train_x,\n",
    "    val_x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking the models performance by storing them into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_f1_score</th>\n",
       "      <th>test_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>PassiveAggressiveClassifier(C=1.0, average=Fal...</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990072202166065</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9624573378839592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CalibratedClassifierCV(base_estimator=None, cv...</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>0.9995487364620939</td>\n",
       "      <td>0.990072202166065</td>\n",
       "      <td>0.9983136593591906</td>\n",
       "      <td>0.9624573378839592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>OneVsRestClassifier(estimator=SVC(C=1.0, break...</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>0.9963898916967509</td>\n",
       "      <td>0.990072202166065</td>\n",
       "      <td>0.9863247863247863</td>\n",
       "      <td>0.9621993127147767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990072202166065</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9619377162629756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>CalibratedClassifierCV(base_estimator=None, cv...</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9882671480144405</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9547038327526132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>OneVsRestClassifier(estimator=SVC(C=1.0, break...</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>0.9997743682310469</td>\n",
       "      <td>0.9882671480144405</td>\n",
       "      <td>0.9991546914623838</td>\n",
       "      <td>0.9543859649122808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>CalibratedClassifierCV(base_estimator=None, cv...</td>\n",
       "      <td>HashingVectorizer(alternate_sign=True, analyze...</td>\n",
       "      <td>0.9984205776173285</td>\n",
       "      <td>0.9873646209386282</td>\n",
       "      <td>0.9940728196443691</td>\n",
       "      <td>0.952054794520548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(ExtraTreeClassifier(ccp_alpha=0.0, class_weig...</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9873646209386282</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.950354609929078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(ExtraTreeClassifier(ccp_alpha=0.0, class_weig...</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9873646209386282</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.950354609929078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PassiveAggressiveClassifier(C=1.0, average=Fal...</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9864620938628159</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9477351916376306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>PassiveAggressiveClassifier(C=1.0, average=Fal...</td>\n",
       "      <td>HashingVectorizer(alternate_sign=True, analyze...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9864620938628159</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9477351916376306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>RidgeClassifierCV(alphas=array([ 0.1,  1. , 10...</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9864620938628159</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9477351916376306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>RidgeClassifier(alpha=1.0, class_weight=None, ...</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>0.9972924187725631</td>\n",
       "      <td>0.9864620938628159</td>\n",
       "      <td>0.9897610921501706</td>\n",
       "      <td>0.9477351916376306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>OneVsRestClassifier(estimator=SVC(C=1.0, break...</td>\n",
       "      <td>HashingVectorizer(alternate_sign=True, analyze...</td>\n",
       "      <td>0.993456678700361</td>\n",
       "      <td>0.9864620938628159</td>\n",
       "      <td>0.9749784296807593</td>\n",
       "      <td>0.9473684210526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>RidgeClassifierCV(alphas=array([ 0.1,  1. , 10...</td>\n",
       "      <td>HashingVectorizer(alternate_sign=True, analyze...</td>\n",
       "      <td>0.9997743682310469</td>\n",
       "      <td>0.9855595667870036</td>\n",
       "      <td>0.9991546914623838</td>\n",
       "      <td>0.943661971830986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>OneVsRestClassifier(estimator=LogisticRegressi...</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>0.9975180505415162</td>\n",
       "      <td>0.9855595667870036</td>\n",
       "      <td>0.9906223358908782</td>\n",
       "      <td>0.943661971830986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(DecisionTreeClassifier(ccp_alpha=0.0, class_w...</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9846570397111913</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9390681003584229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(DecisionTreeClassifier(ccp_alpha=0.0, class_w...</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9846570397111913</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9390681003584229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>HashingVectorizer(alternate_sign=True, analyze...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.983754512635379</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.937062937062937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RidgeClassifier(alpha=1.0, class_weight=None, ...</td>\n",
       "      <td>HashingVectorizer(alternate_sign=True, analyze...</td>\n",
       "      <td>0.9943592057761733</td>\n",
       "      <td>0.983754512635379</td>\n",
       "      <td>0.9784296807592753</td>\n",
       "      <td>0.9366197183098591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RidgeClassifier(alpha=1.0, class_weight=None, ...</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.983754512635379</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.935251798561151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>RidgeClassifierCV(alphas=array([ 0.1,  1. , 10...</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.983754512635379</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.935251798561151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BernoulliNB(alpha=1.0, binarize=0.0, class_pri...</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>0.986687725631769</td>\n",
       "      <td>0.9828519855595668</td>\n",
       "      <td>0.9478337754199824</td>\n",
       "      <td>0.9314079422382672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BernoulliNB(alpha=1.0, binarize=0.0, class_pri...</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>0.986687725631769</td>\n",
       "      <td>0.9828519855595668</td>\n",
       "      <td>0.9478337754199824</td>\n",
       "      <td>0.9314079422382672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>0.9995487364620939</td>\n",
       "      <td>0.9819494584837545</td>\n",
       "      <td>0.9983079526226735</td>\n",
       "      <td>0.9290780141843972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(ExtraTreeClassifier(ccp_alpha=0.0, class_weig...</td>\n",
       "      <td>HashingVectorizer(alternate_sign=True, analyze...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.98014440433213</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9197080291970803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(DecisionTreeClassifier(ccp_alpha=0.0, class_w...</td>\n",
       "      <td>HashingVectorizer(alternate_sign=True, analyze...</td>\n",
       "      <td>0.9970667870036101</td>\n",
       "      <td>0.9783393501805054</td>\n",
       "      <td>0.988936170212766</td>\n",
       "      <td>0.916083916083916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(DecisionTreeClassifier(ccp_alpha=0.0, class_w...</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>0.9963898916967509</td>\n",
       "      <td>0.9774368231046932</td>\n",
       "      <td>0.9863247863247863</td>\n",
       "      <td>0.9122807017543859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(DecisionTreeClassifier(ccp_alpha=0.0, class_w...</td>\n",
       "      <td>HashingVectorizer(alternate_sign=True, analyze...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9774368231046932</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9077490774907748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(DecisionTreeClassifier(ccp_alpha=0.0, class_w...</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>0.9799187725631769</td>\n",
       "      <td>0.973826714801444</td>\n",
       "      <td>0.9208888888888889</td>\n",
       "      <td>0.8982456140350877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>([DecisionTreeRegressor(ccp_alpha=0.0, criteri...</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>0.9817238267148014</td>\n",
       "      <td>0.9747292418772563</td>\n",
       "      <td>0.9277430865298839</td>\n",
       "      <td>0.8970588235294118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>OneVsRestClassifier(estimator=LogisticRegressi...</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>0.9749548736462094</td>\n",
       "      <td>0.9747292418772563</td>\n",
       "      <td>0.8969359331476323</td>\n",
       "      <td>0.8955223880597014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(DecisionTreeClassifier(ccp_alpha=0.0, class_w...</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>0.9972924187725631</td>\n",
       "      <td>0.9729241877256317</td>\n",
       "      <td>0.9897785349233391</td>\n",
       "      <td>0.8943661971830986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>([DecisionTreeRegressor(ccp_alpha=0.0, criteri...</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>0.9862364620938628</td>\n",
       "      <td>0.973826714801444</td>\n",
       "      <td>0.9460654288240495</td>\n",
       "      <td>0.8937728937728938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>HashingVectorizer(alternate_sign=True, analyze...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9720216606498195</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8934707903780069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(DecisionTreeClassifier(ccp_alpha=0.0, class_w...</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>0.9846570397111913</td>\n",
       "      <td>0.9720216606498195</td>\n",
       "      <td>0.9404553415061296</td>\n",
       "      <td>0.8904593639575972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>([DecisionTreeRegressor(ccp_alpha=0.0, criteri...</td>\n",
       "      <td>HashingVectorizer(alternate_sign=True, analyze...</td>\n",
       "      <td>0.9848826714801444</td>\n",
       "      <td>0.9720216606498195</td>\n",
       "      <td>0.9407603890362513</td>\n",
       "      <td>0.8880866425992779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(ccp_alpha=0.0, class_w...</td>\n",
       "      <td>HashingVectorizer(alternate_sign=True, analyze...</td>\n",
       "      <td>0.9803700361010831</td>\n",
       "      <td>0.9711191335740073</td>\n",
       "      <td>0.9223907225691348</td>\n",
       "      <td>0.8857142857142858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>OneVsRestClassifier(estimator=LogisticRegressi...</td>\n",
       "      <td>HashingVectorizer(alternate_sign=True, analyze...</td>\n",
       "      <td>0.9758574007220217</td>\n",
       "      <td>0.9720216606498195</td>\n",
       "      <td>0.9021043000914915</td>\n",
       "      <td>0.8838951310861423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9657039711191335</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.87248322147651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9657039711191335</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8642857142857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>HashingVectorizer(alternate_sign=True, analyze...</td>\n",
       "      <td>0.9305054151624549</td>\n",
       "      <td>0.9250902527075813</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.6103286384976526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>0.930956678700361</td>\n",
       "      <td>0.9232851985559567</td>\n",
       "      <td>0.6514806378132119</td>\n",
       "      <td>0.5971563981042654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>0.9189981949458483</td>\n",
       "      <td>0.9151624548736462</td>\n",
       "      <td>0.5648484848484848</td>\n",
       "      <td>0.5346534653465346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BernoulliNB(alpha=1.0, binarize=0.0, class_pri...</td>\n",
       "      <td>HashingVectorizer(alternate_sign=True, analyze...</td>\n",
       "      <td>0.8664259927797834</td>\n",
       "      <td>0.8664259927797834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           classifier  \\\n",
       "25  PassiveAggressiveClassifier(C=1.0, average=Fal...   \n",
       "22  CalibratedClassifierCV(base_estimator=None, cv...   \n",
       "37  OneVsRestClassifier(estimator=SVC(C=1.0, break...   \n",
       "34  SGDClassifier(alpha=0.0001, average=False, cla...   \n",
       "21  CalibratedClassifierCV(base_estimator=None, cv...   \n",
       "36  OneVsRestClassifier(estimator=SVC(C=1.0, break...   \n",
       "23  CalibratedClassifierCV(base_estimator=None, cv...   \n",
       "12  (ExtraTreeClassifier(ccp_alpha=0.0, class_weig...   \n",
       "13  (ExtraTreeClassifier(ccp_alpha=0.0, class_weig...   \n",
       "24  PassiveAggressiveClassifier(C=1.0, average=Fal...   \n",
       "26  PassiveAggressiveClassifier(C=1.0, average=Fal...   \n",
       "31  RidgeClassifierCV(alphas=array([ 0.1,  1. , 10...   \n",
       "28  RidgeClassifier(alpha=1.0, class_weight=None, ...   \n",
       "38  OneVsRestClassifier(estimator=SVC(C=1.0, break...   \n",
       "32  RidgeClassifierCV(alphas=array([ 0.1,  1. , 10...   \n",
       "39  OneVsRestClassifier(estimator=LogisticRegressi...   \n",
       "3   (DecisionTreeClassifier(ccp_alpha=0.0, class_w...   \n",
       "4   (DecisionTreeClassifier(ccp_alpha=0.0, class_w...   \n",
       "35  SGDClassifier(alpha=0.0001, average=False, cla...   \n",
       "29  RidgeClassifier(alpha=1.0, class_weight=None, ...   \n",
       "27  RidgeClassifier(alpha=1.0, class_weight=None, ...   \n",
       "30  RidgeClassifierCV(alphas=array([ 0.1,  1. , 10...   \n",
       "0   BernoulliNB(alpha=1.0, binarize=0.0, class_pri...   \n",
       "1   BernoulliNB(alpha=1.0, binarize=0.0, class_pri...   \n",
       "33  SGDClassifier(alpha=0.0001, average=False, cla...   \n",
       "14  (ExtraTreeClassifier(ccp_alpha=0.0, class_weig...   \n",
       "11  (DecisionTreeClassifier(ccp_alpha=0.0, class_w...   \n",
       "9   (DecisionTreeClassifier(ccp_alpha=0.0, class_w...   \n",
       "5   (DecisionTreeClassifier(ccp_alpha=0.0, class_w...   \n",
       "6   (DecisionTreeClassifier(ccp_alpha=0.0, class_w...   \n",
       "15  ([DecisionTreeRegressor(ccp_alpha=0.0, criteri...   \n",
       "40  OneVsRestClassifier(estimator=LogisticRegressi...   \n",
       "10  (DecisionTreeClassifier(ccp_alpha=0.0, class_w...   \n",
       "16  ([DecisionTreeRegressor(ccp_alpha=0.0, criteri...   \n",
       "20  DecisionTreeClassifier(ccp_alpha=0.0, class_we...   \n",
       "7   (DecisionTreeClassifier(ccp_alpha=0.0, class_w...   \n",
       "17  ([DecisionTreeRegressor(ccp_alpha=0.0, criteri...   \n",
       "8   (DecisionTreeClassifier(ccp_alpha=0.0, class_w...   \n",
       "41  OneVsRestClassifier(estimator=LogisticRegressi...   \n",
       "18  DecisionTreeClassifier(ccp_alpha=0.0, class_we...   \n",
       "19  DecisionTreeClassifier(ccp_alpha=0.0, class_we...   \n",
       "44  KNeighborsClassifier(algorithm='auto', leaf_si...   \n",
       "42  KNeighborsClassifier(algorithm='auto', leaf_si...   \n",
       "43  KNeighborsClassifier(algorithm='auto', leaf_si...   \n",
       "2   BernoulliNB(alpha=1.0, binarize=0.0, class_pri...   \n",
       "\n",
       "                                           vectorizer         train_score  \\\n",
       "25  TfidfVectorizer(analyzer='word', binary=False,...                 1.0   \n",
       "22  TfidfVectorizer(analyzer='word', binary=False,...  0.9995487364620939   \n",
       "37  TfidfVectorizer(analyzer='word', binary=False,...  0.9963898916967509   \n",
       "34  TfidfVectorizer(analyzer='word', binary=False,...                 1.0   \n",
       "21  CountVectorizer(analyzer='word', binary=False,...                 1.0   \n",
       "36  CountVectorizer(analyzer='word', binary=False,...  0.9997743682310469   \n",
       "23  HashingVectorizer(alternate_sign=True, analyze...  0.9984205776173285   \n",
       "12  CountVectorizer(analyzer='word', binary=False,...                 1.0   \n",
       "13  TfidfVectorizer(analyzer='word', binary=False,...                 1.0   \n",
       "24  CountVectorizer(analyzer='word', binary=False,...                 1.0   \n",
       "26  HashingVectorizer(alternate_sign=True, analyze...                 1.0   \n",
       "31  TfidfVectorizer(analyzer='word', binary=False,...                 1.0   \n",
       "28  TfidfVectorizer(analyzer='word', binary=False,...  0.9972924187725631   \n",
       "38  HashingVectorizer(alternate_sign=True, analyze...   0.993456678700361   \n",
       "32  HashingVectorizer(alternate_sign=True, analyze...  0.9997743682310469   \n",
       "39  CountVectorizer(analyzer='word', binary=False,...  0.9975180505415162   \n",
       "3   CountVectorizer(analyzer='word', binary=False,...                 1.0   \n",
       "4   TfidfVectorizer(analyzer='word', binary=False,...                 1.0   \n",
       "35  HashingVectorizer(alternate_sign=True, analyze...                 1.0   \n",
       "29  HashingVectorizer(alternate_sign=True, analyze...  0.9943592057761733   \n",
       "27  CountVectorizer(analyzer='word', binary=False,...                 1.0   \n",
       "30  CountVectorizer(analyzer='word', binary=False,...                 1.0   \n",
       "0   CountVectorizer(analyzer='word', binary=False,...   0.986687725631769   \n",
       "1   TfidfVectorizer(analyzer='word', binary=False,...   0.986687725631769   \n",
       "33  CountVectorizer(analyzer='word', binary=False,...  0.9995487364620939   \n",
       "14  HashingVectorizer(alternate_sign=True, analyze...                 1.0   \n",
       "11  HashingVectorizer(alternate_sign=True, analyze...  0.9970667870036101   \n",
       "9   CountVectorizer(analyzer='word', binary=False,...  0.9963898916967509   \n",
       "5   HashingVectorizer(alternate_sign=True, analyze...                 1.0   \n",
       "6   CountVectorizer(analyzer='word', binary=False,...  0.9799187725631769   \n",
       "15  CountVectorizer(analyzer='word', binary=False,...  0.9817238267148014   \n",
       "40  TfidfVectorizer(analyzer='word', binary=False,...  0.9749548736462094   \n",
       "10  TfidfVectorizer(analyzer='word', binary=False,...  0.9972924187725631   \n",
       "16  TfidfVectorizer(analyzer='word', binary=False,...  0.9862364620938628   \n",
       "20  HashingVectorizer(alternate_sign=True, analyze...                 1.0   \n",
       "7   TfidfVectorizer(analyzer='word', binary=False,...  0.9846570397111913   \n",
       "17  HashingVectorizer(alternate_sign=True, analyze...  0.9848826714801444   \n",
       "8   HashingVectorizer(alternate_sign=True, analyze...  0.9803700361010831   \n",
       "41  HashingVectorizer(alternate_sign=True, analyze...  0.9758574007220217   \n",
       "18  CountVectorizer(analyzer='word', binary=False,...                 1.0   \n",
       "19  TfidfVectorizer(analyzer='word', binary=False,...                 1.0   \n",
       "44  HashingVectorizer(alternate_sign=True, analyze...  0.9305054151624549   \n",
       "42  CountVectorizer(analyzer='word', binary=False,...   0.930956678700361   \n",
       "43  TfidfVectorizer(analyzer='word', binary=False,...  0.9189981949458483   \n",
       "2   HashingVectorizer(alternate_sign=True, analyze...  0.8664259927797834   \n",
       "\n",
       "            test_score      train_f1_score       test_f1_score  \n",
       "25   0.990072202166065                 1.0  0.9624573378839592  \n",
       "22   0.990072202166065  0.9983136593591906  0.9624573378839592  \n",
       "37   0.990072202166065  0.9863247863247863  0.9621993127147767  \n",
       "34   0.990072202166065                 1.0  0.9619377162629756  \n",
       "21  0.9882671480144405                 1.0  0.9547038327526132  \n",
       "36  0.9882671480144405  0.9991546914623838  0.9543859649122808  \n",
       "23  0.9873646209386282  0.9940728196443691   0.952054794520548  \n",
       "12  0.9873646209386282                 1.0   0.950354609929078  \n",
       "13  0.9873646209386282                 1.0   0.950354609929078  \n",
       "24  0.9864620938628159                 1.0  0.9477351916376306  \n",
       "26  0.9864620938628159                 1.0  0.9477351916376306  \n",
       "31  0.9864620938628159                 1.0  0.9477351916376306  \n",
       "28  0.9864620938628159  0.9897610921501706  0.9477351916376306  \n",
       "38  0.9864620938628159  0.9749784296807593  0.9473684210526316  \n",
       "32  0.9855595667870036  0.9991546914623838   0.943661971830986  \n",
       "39  0.9855595667870036  0.9906223358908782   0.943661971830986  \n",
       "3   0.9846570397111913                 1.0  0.9390681003584229  \n",
       "4   0.9846570397111913                 1.0  0.9390681003584229  \n",
       "35   0.983754512635379                 1.0   0.937062937062937  \n",
       "29   0.983754512635379  0.9784296807592753  0.9366197183098591  \n",
       "27   0.983754512635379                 1.0   0.935251798561151  \n",
       "30   0.983754512635379                 1.0   0.935251798561151  \n",
       "0   0.9828519855595668  0.9478337754199824  0.9314079422382672  \n",
       "1   0.9828519855595668  0.9478337754199824  0.9314079422382672  \n",
       "33  0.9819494584837545  0.9983079526226735  0.9290780141843972  \n",
       "14    0.98014440433213                 1.0  0.9197080291970803  \n",
       "11  0.9783393501805054   0.988936170212766   0.916083916083916  \n",
       "9   0.9774368231046932  0.9863247863247863  0.9122807017543859  \n",
       "5   0.9774368231046932                 1.0  0.9077490774907748  \n",
       "6    0.973826714801444  0.9208888888888889  0.8982456140350877  \n",
       "15  0.9747292418772563  0.9277430865298839  0.8970588235294118  \n",
       "40  0.9747292418772563  0.8969359331476323  0.8955223880597014  \n",
       "10  0.9729241877256317  0.9897785349233391  0.8943661971830986  \n",
       "16   0.973826714801444  0.9460654288240495  0.8937728937728938  \n",
       "20  0.9720216606498195                 1.0  0.8934707903780069  \n",
       "7   0.9720216606498195  0.9404553415061296  0.8904593639575972  \n",
       "17  0.9720216606498195  0.9407603890362513  0.8880866425992779  \n",
       "8   0.9711191335740073  0.9223907225691348  0.8857142857142858  \n",
       "41  0.9720216606498195  0.9021043000914915  0.8838951310861423  \n",
       "18  0.9657039711191335                 1.0    0.87248322147651  \n",
       "19  0.9657039711191335                 1.0  0.8642857142857143  \n",
       "44  0.9250902527075813                0.65  0.6103286384976526  \n",
       "42  0.9232851985559567  0.6514806378132119  0.5971563981042654  \n",
       "43  0.9151624548736462  0.5648484848484848  0.5346534653465346  \n",
       "2   0.8664259927797834                 0.0                 0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_summary = pd.DataFrame(models_summary, columns = ['classifier', 'vectorizer', 'train_score', \n",
    "                                                        'test_score', 'train_f1_score', 'test_f1_score'])\n",
    "models_summary.sort_values(['test_f1_score', 'train_f1_score'],  ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the top 5 models to be used for Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results we can see that the TFIDF Vectorizer is performing well when compared to others. So, will use TFIDF Vectorizer for vectorizing the data and will take 5 unique classifiers from above results to be used for stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 models : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('PassiveAggressiveClassifier',\n",
       "  PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
       "                              early_stopping=False, fit_intercept=True,\n",
       "                              loss='hinge', max_iter=1000, n_iter_no_change=5,\n",
       "                              n_jobs=-1, random_state=3, shuffle=True, tol=0.001,\n",
       "                              validation_fraction=0.1, verbose=0,\n",
       "                              warm_start=False)),\n",
       " ('CalibratedClassifierCV',\n",
       "  CalibratedClassifierCV(base_estimator=None, cv=None, method='sigmoid')),\n",
       " ('OneVsRestClassifier',\n",
       "  OneVsRestClassifier(estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                    class_weight=None, coef0=0.0,\n",
       "                                    decision_function_shape='ovr', degree=3,\n",
       "                                    gamma='scale', kernel='linear', max_iter=-1,\n",
       "                                    probability=True, random_state=3,\n",
       "                                    shrinking=True, tol=0.001, verbose=False),\n",
       "                      n_jobs=None)),\n",
       " ('SGDClassifier',\n",
       "  SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "                l1_ratio=0.15, learning_rate='optimal', loss='modified_huber',\n",
       "                max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',\n",
       "                power_t=0.5, random_state=3, shuffle=True, tol=0.001,\n",
       "                validation_fraction=0.1, verbose=0, warm_start=False)),\n",
       " ('ExtraTreesClassifier',\n",
       "  ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "                       oob_score=False, random_state=3, verbose=0,\n",
       "                       warm_start=False))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_models = models_summary.sort_values(['test_f1_score', 'train_f1_score'],  ascending = False) # Storing the sorted data\n",
    "\n",
    "stacking_models = []\n",
    "for model in top_models.values:\n",
    "    \"\"\"\n",
    "    generating the stacking models with data stored in tuples with name and classifier object\n",
    "    \n",
    "    Will take the unique classifiers from top performing models\n",
    "    \"\"\"\n",
    "    \n",
    "    model_name = model[0].__class__.__name__.replace('(', '').replace(')', '')\n",
    "    current_stack = [x[0][0] for x in zip(stacking_models)]\n",
    "    \n",
    "    if model_name in current_stack:\n",
    "        #checking whether the model is in the stacking list or not\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        stacking_models.append((model_name, model[0]))\n",
    "        \n",
    "    if len(stacking_models)>=5:\n",
    "        \n",
    "        #reading only 5 models\n",
    "        break\n",
    "\n",
    "print(\"Top 5 models : \", sep = '\\n')\n",
    "stacking_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the data with vectorizer and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainging with \n",
      "['PassiveAggressiveClassifier', 'CalibratedClassifierCV', 'OneVsRestClassifier', 'SGDClassifier', 'ExtraTreesClassifier']\n",
      "Training Score :  0.9995487364620939\n"
     ]
    }
   ],
   "source": [
    "vectorized_ouput = vectorize(train_x.features)\n",
    "\n",
    "trained_tfidf_vectorizer = vectorized_ouput['tfidf_vectorizer']\n",
    "\n",
    "model_trained = train_model(vectorized_ouput['features'], train_x.label, stacking_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Model performance on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score :  0.9657534246575342\n"
     ]
    }
   ],
   "source": [
    "vectorized_ouput = vectorize(val_x.features, train = False, tfidf_vectorizer = trained_tfidf_vectorizer)\n",
    "print(\"Test Score : \", f1_score(model_trained['model'].predict(vectorized_ouput['features']), val_x.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the stacked model and the top model given almost same performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[957,   7],\n",
       "       [  3, 141]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Confusion Matrix : \", sep = '\\n')\n",
    "confusion_matrix(model_trained['model'].predict(vectorized_ouput['features']), val_x.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize Test data\n",
    "test_vectorized_ouput = vectorize(test_data.features, train = False, tfidf_vectorizer = trained_tfidf_vectorizer)\n",
    "#Predicting on Test data\n",
    "test_data['predicted'] = model_trained['model'].predict(test_vectorized_ouput['features']).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Result</th>\n",
       "      <th>sms</th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>spam</td>\n",
       "      <td>Your free ringtone is waiting to be collected....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>your free ringtone is waiting to be collected ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>125</td>\n",
       "      <td>ham</td>\n",
       "      <td>I am going to sao mu today. Will be done only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>am going to sao mu today will be done only at 12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>348</td>\n",
       "      <td>ham</td>\n",
       "      <td>Dis is yijue. I jus saw ur mail. In case huimi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dis is yijue jus saw ur mail in case huiming h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>568</td>\n",
       "      <td>ham</td>\n",
       "      <td>Oooh bed ridden ey? What are YOU thinking of?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>oooh bed ridden ey what are you thinking of</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>599</td>\n",
       "      <td>spam</td>\n",
       "      <td>You have an important customer service announc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>you have an important customer service announc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>734</td>\n",
       "      <td>ham</td>\n",
       "      <td>Lol you won't feel bad when I use her money to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lol you won't feel bad when use her money to t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>941</td>\n",
       "      <td>ham</td>\n",
       "      <td>Better. Made up for Friday and stuffed myself ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>better made up for friday and stuffed myself l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>1100</td>\n",
       "      <td>ham</td>\n",
       "      <td>NO GIFTS!! You trying to get me to throw mysel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no gifts you trying to get me to throw myself ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>1155</td>\n",
       "      <td>spam</td>\n",
       "      <td>1000's of girls many local 2 u who r virgins 2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000's of girls many local who virgins this re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>1350</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nothing much, chillin at home. Any super bowl ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nothing much chillin at home any super bowl plan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>1532</td>\n",
       "      <td>ham</td>\n",
       "      <td>I think chennai well settled?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>think chennai well settled</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>1725</td>\n",
       "      <td>ham</td>\n",
       "      <td>Hi Jon, Pete here, Ive bin 2 Spain recently &amp; ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hi jon pete here ive bin spain recently hav su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>1921</td>\n",
       "      <td>ham</td>\n",
       "      <td>Yar i wanted 2 scold u yest but late already.....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yar wanted scold yest but late already where g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>1966</td>\n",
       "      <td>ham</td>\n",
       "      <td>Honeybee Said: *I'm d Sweetest in d World* God...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>honeybee said i'm sweetest in world god laughe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186</th>\n",
       "      <td>2187</td>\n",
       "      <td>ham</td>\n",
       "      <td>Purity of friendship between two is not about ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>purity of friendship between two is not about ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2413</th>\n",
       "      <td>2414</td>\n",
       "      <td>spam</td>\n",
       "      <td>I don't know u and u don't know me. Send CHAT ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>don't know and don't know me send chat to 8668...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2435</th>\n",
       "      <td>2436</td>\n",
       "      <td>ham</td>\n",
       "      <td>Uncle boye. I need movies oh. Guide me. Plus y...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uncle boye need movies oh guide me plus you kn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2820</th>\n",
       "      <td>2821</td>\n",
       "      <td>ham</td>\n",
       "      <td>Don't forget who owns you and who's private pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>don't forget who owns you and who's private pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3107</th>\n",
       "      <td>3108</td>\n",
       "      <td>ham</td>\n",
       "      <td>I had been hoping i would not have to send you...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>had been hoping would not have to send you thi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3407</th>\n",
       "      <td>3408</td>\n",
       "      <td>ham</td>\n",
       "      <td>HEY DAS COOL... IKNOW ALL 2 WELLDA PERIL OF ST...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hey das cool iknow all wellda peril of student...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3637</th>\n",
       "      <td>3638</td>\n",
       "      <td>ham</td>\n",
       "      <td>ME 2 BABE I FEEL THE SAME LETS JUST 4GET ABOUT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>me babe feel the same lets just 4get about it ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3799</th>\n",
       "      <td>3800</td>\n",
       "      <td>ham</td>\n",
       "      <td>Feb  &amp;lt;#&amp;gt;  is \"I LOVE U\" day. Send dis to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>feb lt gt is love day send dis to all ur value...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3824</th>\n",
       "      <td>3825</td>\n",
       "      <td>ham</td>\n",
       "      <td>Please protect yourself from e-threats. SIB ne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>please protect yourself from threats sib never...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3841</th>\n",
       "      <td>3842</td>\n",
       "      <td>ham</td>\n",
       "      <td>HEY MATE! HOWS U HONEY?DID U AVE GOOD HOLIDAY?...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hey mate hows honey did ave good holiday gimmi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4047</th>\n",
       "      <td>4048</td>\n",
       "      <td>spam</td>\n",
       "      <td>Win a 1000 cash prize or a prize worth 5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>win 1000 cash prize or prize worth 5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4211</th>\n",
       "      <td>4212</td>\n",
       "      <td>ham</td>\n",
       "      <td>No da:)he is stupid da..always sending like th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no da he is stupid da always sending like this...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4240</th>\n",
       "      <td>4241</td>\n",
       "      <td>ham</td>\n",
       "      <td>Sez, hows u &amp; de arab boy? Hope u r all good g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sez hows de arab boy hope all good give my lov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4529</th>\n",
       "      <td>4530</td>\n",
       "      <td>ham</td>\n",
       "      <td>HOW ARE U? I HAVE MISSED U! I HAVENT BEEN UP 2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>how are have missed havent been up much bit bo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5050</th>\n",
       "      <td>5051</td>\n",
       "      <td>ham</td>\n",
       "      <td>Edison has rightly said, \"A fool can ask more ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>edison has rightly said fool can ask more ques...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5236</th>\n",
       "      <td>5237</td>\n",
       "      <td>ham</td>\n",
       "      <td>Your opinion about me? 1. Over 2. Jada 3. Kusr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>your opinion about me over jada kusruthi lovab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5365</th>\n",
       "      <td>5366</td>\n",
       "      <td>spam</td>\n",
       "      <td>Camera - You are awarded a SiPix Digital Camer...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>camera you are awarded sipix digital camera ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>5548</td>\n",
       "      <td>spam</td>\n",
       "      <td>Had your contract mobile 11 Mnths? Latest Moto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>had your contract mobile 11 mnths latest motor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        No Result                                                sms  label  \\\n",
       "95      96   spam  Your free ringtone is waiting to be collected....    NaN   \n",
       "124    125    ham  I am going to sao mu today. Will be done only ...    NaN   \n",
       "347    348    ham  Dis is yijue. I jus saw ur mail. In case huimi...    NaN   \n",
       "567    568    ham      Oooh bed ridden ey? What are YOU thinking of?    NaN   \n",
       "598    599   spam  You have an important customer service announc...    NaN   \n",
       "733    734    ham  Lol you won't feel bad when I use her money to...    NaN   \n",
       "940    941    ham  Better. Made up for Friday and stuffed myself ...    NaN   \n",
       "1099  1100    ham  NO GIFTS!! You trying to get me to throw mysel...    NaN   \n",
       "1154  1155   spam  1000's of girls many local 2 u who r virgins 2...    NaN   \n",
       "1349  1350    ham  Nothing much, chillin at home. Any super bowl ...    NaN   \n",
       "1531  1532    ham                      I think chennai well settled?    NaN   \n",
       "1724  1725    ham  Hi Jon, Pete here, Ive bin 2 Spain recently & ...    NaN   \n",
       "1920  1921    ham  Yar i wanted 2 scold u yest but late already.....    NaN   \n",
       "1965  1966    ham  Honeybee Said: *I'm d Sweetest in d World* God...    NaN   \n",
       "2186  2187    ham  Purity of friendship between two is not about ...    NaN   \n",
       "2413  2414   spam  I don't know u and u don't know me. Send CHAT ...    NaN   \n",
       "2435  2436    ham  Uncle boye. I need movies oh. Guide me. Plus y...    NaN   \n",
       "2820  2821    ham  Don't forget who owns you and who's private pr...    NaN   \n",
       "3107  3108    ham  I had been hoping i would not have to send you...    NaN   \n",
       "3407  3408    ham  HEY DAS COOL... IKNOW ALL 2 WELLDA PERIL OF ST...    NaN   \n",
       "3637  3638    ham  ME 2 BABE I FEEL THE SAME LETS JUST 4GET ABOUT...    NaN   \n",
       "3799  3800    ham  Feb  &lt;#&gt;  is \"I LOVE U\" day. Send dis to...    NaN   \n",
       "3824  3825    ham  Please protect yourself from e-threats. SIB ne...    NaN   \n",
       "3841  3842    ham  HEY MATE! HOWS U HONEY?DID U AVE GOOD HOLIDAY?...    NaN   \n",
       "4047  4048   spam      Win a 1000 cash prize or a prize worth 5000    NaN   \n",
       "4211  4212    ham  No da:)he is stupid da..always sending like th...    NaN   \n",
       "4240  4241    ham  Sez, hows u & de arab boy? Hope u r all good g...    NaN   \n",
       "4529  4530    ham  HOW ARE U? I HAVE MISSED U! I HAVENT BEEN UP 2...    NaN   \n",
       "5050  5051    ham  Edison has rightly said, \"A fool can ask more ...    NaN   \n",
       "5236  5237    ham  Your opinion about me? 1. Over 2. Jada 3. Kusr...    NaN   \n",
       "5365  5366   spam  Camera - You are awarded a SiPix Digital Camer...    NaN   \n",
       "5547  5548   spam  Had your contract mobile 11 Mnths? Latest Moto...    NaN   \n",
       "\n",
       "                                               features  predicted  \n",
       "95    your free ringtone is waiting to be collected ...          1  \n",
       "124    am going to sao mu today will be done only at 12          0  \n",
       "347   dis is yijue jus saw ur mail in case huiming h...          0  \n",
       "567         oooh bed ridden ey what are you thinking of          0  \n",
       "598   you have an important customer service announc...          1  \n",
       "733   lol you won't feel bad when use her money to t...          0  \n",
       "940   better made up for friday and stuffed myself l...          0  \n",
       "1099  no gifts you trying to get me to throw myself ...          0  \n",
       "1154  1000's of girls many local who virgins this re...          1  \n",
       "1349   nothing much chillin at home any super bowl plan          0  \n",
       "1531                         think chennai well settled          0  \n",
       "1724  hi jon pete here ive bin spain recently hav su...          0  \n",
       "1920  yar wanted scold yest but late already where g...          0  \n",
       "1965  honeybee said i'm sweetest in world god laughe...          0  \n",
       "2186  purity of friendship between two is not about ...          0  \n",
       "2413  don't know and don't know me send chat to 8668...          1  \n",
       "2435  uncle boye need movies oh guide me plus you kn...          0  \n",
       "2820  don't forget who owns you and who's private pr...          0  \n",
       "3107  had been hoping would not have to send you thi...          0  \n",
       "3407  hey das cool iknow all wellda peril of student...          0  \n",
       "3637  me babe feel the same lets just 4get about it ...          0  \n",
       "3799  feb lt gt is love day send dis to all ur value...          0  \n",
       "3824  please protect yourself from threats sib never...          0  \n",
       "3841  hey mate hows honey did ave good holiday gimmi...          0  \n",
       "4047            win 1000 cash prize or prize worth 5000          1  \n",
       "4211  no da he is stupid da always sending like this...          0  \n",
       "4240  sez hows de arab boy hope all good give my lov...          0  \n",
       "4529  how are have missed havent been up much bit bo...          0  \n",
       "5050  edison has rightly said fool can ask more ques...          0  \n",
       "5236  your opinion about me over jada kusruthi lovab...          0  \n",
       "5365  camera you are awarded sipix digital camera ca...          1  \n",
       "5547  had your contract mobile 11 mnths latest motor...          1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['Result'] = test_data['predicted'].replace(1, 'spam').replace(0, 'ham')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing predicted results into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[['No', 'Result']].to_csv('submission_v1.csv', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
