{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic:Machine Learning From Disaster "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Titanic Dataset with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Libriaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark #pyspark package\n",
    "from pyspark.sql import SparkSession #sparksession object to build the spark session\n",
    "from pyspark.sql.types import * #spark-sql datatypes\n",
    "from pyspark.sql.functions import * #spark-sql functions\n",
    "import os # os module to work with the files\n",
    "import matplotlib.pyplot as plt # matplotlib.pyplot for plotting \n",
    "import seaborn as sns # seaborn for aditional plotting options\n",
    "from statsmodels.graphics.mosaicplot import mosaic # for mosaic Plot\n",
    "from pyspark.ml.feature import (StringIndexer, OneHotEncoder, \n",
    "                                VectorAssembler, StandardScaler) #To Process the data for ML Model\n",
    "from pyspark.ml.classification import RandomForestClassifier #Random Forest Clasiifier Model for Prediction\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator #For Evaluating the Classification Model\n",
    "from pyspark.ml import Pipeline #To define the model pipeline\n",
    "\n",
    "#to plot the images in the jupyter notebook\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://NarendraBabuOggu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Titanic Dataset Analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1fae9e08eb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create spark session\n",
    "spark = (SparkSession.builder\n",
    "         #.config(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "         .appName(\"Titanic Dataset Analysis\").master('local')\n",
    "         .getOrCreate())\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender_submission.csv',\n",
       " 'pyspark_rf_submission',\n",
       " 'pyspark_rf_submission.csv',\n",
       " 'PyTorchTitanicModel.pt',\n",
       " 'PyTorchTitanicSubmission.csv',\n",
       " 'test.csv',\n",
       " 'test_preprocessed.csv',\n",
       " 'train.csv',\n",
       " 'train_preprocessed.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH = '../DATASETS/titanic/'\n",
    "os.listdir(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read train DataFrame from CSV File\n",
    "train = spark.read.csv(DATASET_PATH + 'train.csv', header = True, inferSchema = True)\n",
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read Test DataFrame from CSV File\n",
    "test = spark.read.csv(DATASET_PATH + 'test.csv', header = True, inferSchema = True)\n",
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using few columns at a time for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+------------------+------------------------------------------------+------+------------------+\n",
      "|summary|PassengerId      |Survived           |Pclass            |Name                                            |Sex   |Age               |\n",
      "+-------+-----------------+-------------------+------------------+------------------------------------------------+------+------------------+\n",
      "|count  |891              |891                |891               |891                                             |891   |714               |\n",
      "|mean   |446.0            |0.3838383838383838 |2.308641975308642 |null                                            |null  |29.69911764705882 |\n",
      "|stddev |257.3538420152301|0.48659245426485753|0.8360712409770491|null                                            |null  |14.526497332334035|\n",
      "|min    |1                |0                  |1                 |\"Andersson, Mr. August Edvard (\"\"Wennerstrom\"\")\"|female|0.42              |\n",
      "|max    |891              |1                  |3                 |van Melkebeke, Mr. Philemon                     |male  |80.0              |\n",
      "+-------+-----------------+-------------------+------------------+------------------------------------------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.describe(train.columns[:6]).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|summary|SibSp             |Parch              |Ticket            |Fare             |Cabin|Embarked|\n",
      "+-------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|count  |891               |891                |891               |891              |204  |889     |\n",
      "|mean   |0.5230078563411896|0.38159371492704824|260318.54916792738|32.2042079685746 |null |null    |\n",
      "|stddev |1.1027434322934315|0.8060572211299488 |471609.26868834975|49.69342859718089|null |null    |\n",
      "|min    |0                 |0                  |110152            |0.0              |A10  |C       |\n",
      "|max    |8                 |6                  |WE/P 5735         |512.3292         |T    |S       |\n",
      "+-------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.describe(train.columns[6:]).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------------------------------------+------+------------------+\n",
      "|summary|PassengerId       |Pclass            |Name                                     |Sex   |Age               |\n",
      "+-------+------------------+------------------+-----------------------------------------+------+------------------+\n",
      "|count  |418               |418               |418                                      |418   |332               |\n",
      "|mean   |1100.5            |2.2655502392344498|null                                     |null  |30.272590361445783|\n",
      "|stddev |120.81045760473994|0.8418375519640503|null                                     |null  |14.181209235624424|\n",
      "|min    |892               |1                 |\"Assaf Khalil, Mrs. Mariana (Miriam\"\")\"\"\"|female|0.17              |\n",
      "|max    |1309              |3                 |van Billiard, Master. Walter John        |male  |76.0              |\n",
      "+-------+------------------+------------------+-----------------------------------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.describe(test.columns[:5]).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-----+--------+\n",
      "|summary|SibSp             |Parch             |Ticket            |Fare              |Cabin|Embarked|\n",
      "+-------+------------------+------------------+------------------+------------------+-----+--------+\n",
      "|count  |418               |418               |418               |417               |91   |418     |\n",
      "|mean   |0.4473684210526316|0.3923444976076555|223850.98986486485|35.6271884892086  |null |null    |\n",
      "|stddev |0.8967595611217135|0.9814288785371694|369523.7764694362 |55.907576179973844|null |null    |\n",
      "|min    |0                 |0                 |110469            |0.0               |A11  |C       |\n",
      "|max    |8                 |9                 |W.E.P. 5734       |512.3292          |G6   |S       |\n",
      "+-------+------------------+------------------+------------------+------------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.describe(test.columns[5:]).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5, truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------------------------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "|PassengerId|Pclass|Name                                        |Sex   |Age |SibSp|Parch|Ticket   |Fare   |Cabin|Embarked|\n",
      "+-----------+------+--------------------------------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "|892        |3     |Kelly, Mr. James                            |male  |34.5|0    |0    |330911   |7.8292 |null |Q       |\n",
      "|893        |3     |Wilkes, Mrs. James (Ellen Needs)            |female|47.0|1    |0    |363272   |7.0    |null |S       |\n",
      "|894        |2     |Myles, Mr. Thomas Francis                   |male  |62.0|0    |0    |240276   |9.6875 |null |Q       |\n",
      "|895        |3     |Wirz, Mr. Albert                            |male  |27.0|0    |0    |315154   |8.6625 |null |S       |\n",
      "|896        |3     |Hirvonen, Mrs. Alexander (Helga E Lindqvist)|female|22.0|1    |1    |3101298  |12.2875|null |S       |\n",
      "|897        |3     |Svensson, Mr. Johan Cervin                  |male  |14.0|0    |0    |7538     |9.225  |null |S       |\n",
      "|898        |3     |Connolly, Miss. Kate                        |female|30.0|0    |0    |330972   |7.6292 |null |Q       |\n",
      "|899        |2     |Caldwell, Mr. Albert Francis                |male  |26.0|1    |1    |248738   |29.0   |null |S       |\n",
      "|900        |3     |Abrahim, Mrs. Joseph (Sophie Halaut Easu)   |female|18.0|0    |0    |2657     |7.2292 |null |C       |\n",
      "|901        |3     |Davies, Mr. John Samuel                     |male  |21.0|2    |0    |A/4 48871|24.15  |null |S       |\n",
      "+-----------+------+--------------------------------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining Train and Test DataFrames into a Single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+---------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name                                               |Sex   |Age |SibSp|Parch|Ticket          |Fare   |Cabin|Embarked|\n",
      "+-----------+--------+------+---------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|1          |0       |3     |Braund, Mr. Owen Harris                            |male  |22.0|1    |0    |A/5 21171       |7.25   |null |S       |\n",
      "|2          |1       |1     |Cumings, Mrs. John Bradley (Florence Briggs Thayer)|female|38.0|1    |0    |PC 17599        |71.2833|C85  |C       |\n",
      "|3          |1       |3     |Heikkinen, Miss. Laina                             |female|26.0|0    |0    |STON/O2. 3101282|7.925  |null |S       |\n",
      "|4          |1       |1     |Futrelle, Mrs. Jacques Heath (Lily May Peel)       |female|35.0|1    |0    |113803          |53.1   |C123 |S       |\n",
      "|5          |0       |3     |Allen, Mr. William Henry                           |male  |35.0|0    |0    |373450          |8.05   |null |S       |\n",
      "|6          |0       |3     |Moran, Mr. James                                   |male  |null|0    |0    |330877          |8.4583 |null |Q       |\n",
      "|7          |0       |1     |McCarthy, Mr. Timothy J                            |male  |54.0|0    |0    |17463           |51.8625|E46  |S       |\n",
      "|8          |0       |3     |Palsson, Master. Gosta Leonard                     |male  |2.0 |3    |1    |349909          |21.075 |null |S       |\n",
      "|9          |1       |3     |Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  |female|27.0|0    |2    |347742          |11.1333|null |S       |\n",
      "|10         |1       |2     |Nasser, Mrs. Nicholas (Adele Achem)                |female|14.0|1    |0    |237736          |30.0708|null |C       |\n",
      "+-----------+--------+------+---------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total = train.unionAll(test.withColumn(\"Survived\", lit(\"null\")).select(train.columns))\n",
    "total.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Title from name Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+\n",
      "|Name                                               |\n",
      "+---------------------------------------------------+\n",
      "|Braund, Mr. Owen Harris                            |\n",
      "|Cumings, Mrs. John Bradley (Florence Briggs Thayer)|\n",
      "|Heikkinen, Miss. Laina                             |\n",
      "|Futrelle, Mrs. Jacques Heath (Lily May Peel)       |\n",
      "|Allen, Mr. William Henry                           |\n",
      "|Moran, Mr. James                                   |\n",
      "|McCarthy, Mr. Timothy J                            |\n",
      "|Palsson, Master. Gosta Leonard                     |\n",
      "|Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  |\n",
      "|Nasser, Mrs. Nicholas (Adele Achem)                |\n",
      "+---------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total.select(\"Name\").show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------------------------+------+\n",
      "|PassengerId|Name                                               |Title |\n",
      "+-----------+---------------------------------------------------+------+\n",
      "|1          |Braund, Mr. Owen Harris                            |Mr    |\n",
      "|2          |Cumings, Mrs. John Bradley (Florence Briggs Thayer)|Mrs   |\n",
      "|3          |Heikkinen, Miss. Laina                             |Miss  |\n",
      "|4          |Futrelle, Mrs. Jacques Heath (Lily May Peel)       |Mrs   |\n",
      "|5          |Allen, Mr. William Henry                           |Mr    |\n",
      "|6          |Moran, Mr. James                                   |Mr    |\n",
      "|7          |McCarthy, Mr. Timothy J                            |Mr    |\n",
      "|8          |Palsson, Master. Gosta Leonard                     |Master|\n",
      "|9          |Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  |Mrs   |\n",
      "|10         |Nasser, Mrs. Nicholas (Adele Achem)                |Mrs   |\n",
      "+-----------+---------------------------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Extracting the Title an replacing the similar words \n",
    "total = (total.withColumn(\"Title\", regexp_extract(col(\"Name\"), \"(,.*\\.)|(\\\\..*) \", 0))\n",
    "         .withColumn(\"Title\", regexp_replace(\"Title\", \",\", \"\"))\n",
    "         .withColumn(\"Title\", regexp_replace(\"Title\", \"\\.\", \"\"))\n",
    "         .withColumn(\"Title\", trim(\"Title\"))\n",
    "         .withColumn(\"Title\", regexp_replace(\"Title\", 'Mlle|Ms', 'Miss'))\n",
    "         .withColumn(\"Title\", regexp_replace(\"Title\", 'Mme', 'Mrs'))\n",
    "         .withColumn(\"Title\", regexp_replace(\"Title\", 'Mrs Martin .*', 'Mrs'))\n",
    "        )\n",
    "total.select(\"PassengerId\", \"Name\", \"Title\").show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Rare Titles and replacing them with 'Rare' Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|       Title|count|\n",
      "+------------+-----+\n",
      "|          Mr|  757|\n",
      "|        Miss|  264|\n",
      "|         Mrs|  198|\n",
      "|      Master|   61|\n",
      "|         Rev|    8|\n",
      "|          Dr|    8|\n",
      "|         Col|    4|\n",
      "|       Major|    2|\n",
      "|        Dona|    1|\n",
      "|         Don|    1|\n",
      "|    Jonkheer|    1|\n",
      "|        Capt|    1|\n",
      "|        Lady|    1|\n",
      "|the Countess|    1|\n",
      "|         Sir|    1|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total.groupby(\"Title\").count().orderBy('count', ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pyspark_env\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:134: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.pyspark.fallback.enabled' does not have an effect on failures in the middle of computation.\n",
      "  An error occurred while calling o138.getResult.\n",
      ": org.apache.spark.SparkException: Exception thrown in awaitResult: \r\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)\r\n",
      "\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:88)\r\n",
      "\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:84)\r\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\r\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 20.0 failed 1 times, most recent failure: Lost task 13.0 in stage 20.0 (TID 221, NarendraBabuOggu, executor driver): org.apache.spark.util.TaskCompletionListenerException: Memory was leaked by query. Memory leaked: (49152)\n",
      "Allocator(toBatchIterator) 0/49152/49152/9223372036854775807 (res/actual/peak/limit)\n",
      "\n",
      "\n",
      "Previous exception in task: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "\tio.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:490)\n",
      "\tio.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:243)\n",
      "\tio.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:233)\n",
      "\tio.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:245)\n",
      "\torg.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:222)\n",
      "\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:240)\n",
      "\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:226)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:118)\n",
      "\tscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:121)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:97)\n",
      "\tscala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tscala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n",
      "\tscala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tscala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tscala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tscala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n",
      "\tscala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tscala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n",
      "\tscala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tscala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n",
      "\torg.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3560)\n",
      "\torg.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2187)\n",
      "\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\torg.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tjava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tjava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tjava.base/java.lang.Thread.run(Thread.java:832)\r\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:145)\r\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:137)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\r\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\r\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2188)\r\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$5(Dataset.scala:3558)\r\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3562)\r\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3539)\r\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\r\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\r\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\r\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1(Dataset.scala:3539)\r\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1$adapted(Dataset.scala:3538)\r\n",
      "\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:130)\r\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n",
      "\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:132)\r\n",
      "\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:127)\r\n",
      "\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:104)\r\n",
      "\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:98)\r\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$1(SocketAuthServer.scala:60)\r\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\r\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:60)\r\n",
      "Caused by: org.apache.spark.util.TaskCompletionListenerException: Memory was leaked by query. Memory leaked: (49152)\n",
      "Allocator(toBatchIterator) 0/49152/49152/9223372036854775807 (res/actual/peak/limit)\n",
      "\n",
      "\n",
      "Previous exception in task: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "\tio.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:490)\n",
      "\tio.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:243)\n",
      "\tio.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:233)\n",
      "\tio.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:245)\n",
      "\torg.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:222)\n",
      "\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:240)\n",
      "\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:226)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:118)\n",
      "\tscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:121)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:97)\n",
      "\tscala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tscala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n",
      "\tscala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tscala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tscala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tscala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n",
      "\tscala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tscala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n",
      "\tscala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tscala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n",
      "\torg.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3560)\n",
      "\torg.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2187)\n",
      "\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\torg.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tjava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tjava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tjava.base/java.lang.Thread.run(Thread.java:832)\r\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:145)\r\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:137)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n",
      "\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o138.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \r\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)\r\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:88)\r\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:84)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 20.0 failed 1 times, most recent failure: Lost task 13.0 in stage 20.0 (TID 221, NarendraBabuOggu, executor driver): org.apache.spark.util.TaskCompletionListenerException: Memory was leaked by query. Memory leaked: (49152)\nAllocator(toBatchIterator) 0/49152/49152/9223372036854775807 (res/actual/peak/limit)\n\n\nPrevious exception in task: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n\tio.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:490)\n\tio.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:243)\n\tio.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:233)\n\tio.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:245)\n\torg.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:222)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:240)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:226)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:118)\n\tscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:121)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:97)\n\tscala.collection.Iterator.foreach(Iterator.scala:941)\n\tscala.collection.Iterator.foreach$(Iterator.scala:941)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n\tscala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tscala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tscala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tscala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n\tscala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tscala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n\tscala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tscala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n\torg.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3560)\n\torg.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2187)\n\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\torg.apache.spark.scheduler.Task.run(Task.scala:127)\n\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tjava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tjava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tjava.base/java.lang.Thread.run(Thread.java:832)\r\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:145)\r\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:137)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2188)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$5(Dataset.scala:3558)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3562)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1$adapted(Dataset.scala:3538)\r\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:130)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:132)\r\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:127)\r\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:104)\r\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:98)\r\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$1(SocketAuthServer.scala:60)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:60)\r\nCaused by: org.apache.spark.util.TaskCompletionListenerException: Memory was leaked by query. Memory leaked: (49152)\nAllocator(toBatchIterator) 0/49152/49152/9223372036854775807 (res/actual/peak/limit)\n\n\nPrevious exception in task: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n\tio.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:490)\n\tio.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:243)\n\tio.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:233)\n\tio.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:245)\n\torg.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:222)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:240)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:226)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:118)\n\tscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:121)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:97)\n\tscala.collection.Iterator.foreach(Iterator.scala:941)\n\tscala.collection.Iterator.foreach$(Iterator.scala:941)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n\tscala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tscala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tscala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tscala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n\tscala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tscala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n\tscala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tscala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n\torg.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3560)\n\torg.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2187)\n\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\torg.apache.spark.scheduler.Task.run(Task.scala:127)\n\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tjava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tjava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tjava.base/java.lang.Thread.run(Thread.java:832)\r\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:145)\r\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:137)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-bc5fcf41b8fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrare_titles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Title\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"count<10\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Title\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Title\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mrare_titles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'|'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrare_titles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrare_titles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pyspark_env\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m                     \u001b[1;31m# Rename columns to avoid duplicated column names.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                     \u001b[0mtmp_column_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'col_{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                     \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtmp_column_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collect_as_arrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                         \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pyspark_env\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py\u001b[0m in \u001b[0;36m_collect_as_arrow\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[1;31m# Join serving thread and raise any exceptions from collectAsArrowToPython\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m             \u001b[0mjsocket_auth_server\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;31m# Separate RecordBatches from batch order indices in results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pyspark_env\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pyspark_env\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pyspark_env\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o138.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \r\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)\r\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:88)\r\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:84)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 20.0 failed 1 times, most recent failure: Lost task 13.0 in stage 20.0 (TID 221, NarendraBabuOggu, executor driver): org.apache.spark.util.TaskCompletionListenerException: Memory was leaked by query. Memory leaked: (49152)\nAllocator(toBatchIterator) 0/49152/49152/9223372036854775807 (res/actual/peak/limit)\n\n\nPrevious exception in task: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n\tio.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:490)\n\tio.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:243)\n\tio.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:233)\n\tio.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:245)\n\torg.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:222)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:240)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:226)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:118)\n\tscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:121)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:97)\n\tscala.collection.Iterator.foreach(Iterator.scala:941)\n\tscala.collection.Iterator.foreach$(Iterator.scala:941)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n\tscala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tscala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tscala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tscala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n\tscala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tscala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n\tscala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tscala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n\torg.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3560)\n\torg.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2187)\n\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\torg.apache.spark.scheduler.Task.run(Task.scala:127)\n\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tjava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tjava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tjava.base/java.lang.Thread.run(Thread.java:832)\r\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:145)\r\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:137)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2188)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$5(Dataset.scala:3558)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3562)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1$adapted(Dataset.scala:3538)\r\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:130)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:132)\r\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:127)\r\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:104)\r\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:98)\r\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$1(SocketAuthServer.scala:60)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:60)\r\nCaused by: org.apache.spark.util.TaskCompletionListenerException: Memory was leaked by query. Memory leaked: (49152)\nAllocator(toBatchIterator) 0/49152/49152/9223372036854775807 (res/actual/peak/limit)\n\n\nPrevious exception in task: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n\tio.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:490)\n\tio.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:243)\n\tio.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:233)\n\tio.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:245)\n\torg.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:222)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:240)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:226)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:118)\n\tscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:121)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:97)\n\tscala.collection.Iterator.foreach(Iterator.scala:941)\n\tscala.collection.Iterator.foreach$(Iterator.scala:941)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n\tscala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tscala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tscala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tscala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n\tscala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tscala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n\tscala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tscala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n\torg.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3560)\n\torg.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2187)\n\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\torg.apache.spark.scheduler.Task.run(Task.scala:127)\n\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tjava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tjava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tjava.base/java.lang.Thread.run(Thread.java:832)\r\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:145)\r\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:137)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n"
     ]
    }
   ],
   "source": [
    "rare_titles = total.groupby(\"Title\").count().filter(\"count<10\").select(\"Title\").toPandas()[\"Title\"].values.tolist()\n",
    "rare_titles = '|'.join(rare_titles)\n",
    "rare_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total.withColumn(\"Title\", regexp_replace(\"Title\", rare_titles, 'Rare'))\n",
    "total.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting SurName from Name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = (total.withColumn(\"SurName\", regexp_extract(\"Name\", '.*,', 0))\n",
    "         .withColumn(\"SurName\", regexp_replace(\"Surname\", ',', ''))\n",
    "         .withColumn(\"SurName\", trim(\"Surname\"))\n",
    "        )\n",
    "total.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Unique Surnames in Data : \", total.select(\"SurName\").distinct().count())\n",
    "total.groupby(\"SurName\").count().orderBy(\"count\", ascending = False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing Family Size Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total.withColumn(\"FamilySize\", col(\"SibSp\") + col(\"Parch\") + lit(1))\n",
    "family_size_df = total.select(\"PassengerId\", \"FamilySize\", \"Survived\").toPandas()\n",
    "family_size_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Plot of Survival details with respect to Family Size\n",
    "sns.countplot(data = family_size_df, x = 'FamilySize', hue = 'Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the categories in the Family Size and Including them in Family Group Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total.withColumn(\"FamilyGroup\", (when(col(\"FamilySize\")==1, 'single')\n",
    "                            .when((col(\"FamilySize\") > 1) & (col(\"FamilySize\")<5), 'small')\n",
    "                            .otherwise('large')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_group_df = total.filter(\"Survived != 'null'\").select(\"FamilyGroup\", \"Survived\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data = family_group_df, x = 'FamilyGroup', hue = 'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mosaic Plot for readability\n",
    "mosaic(family_group_df, ['FamilyGroup', 'Survived']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Extracting Deck Column from The Cabin Details</p>\n",
    "<p>Extracting children and Mother Details</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = (total.withColumn(\"Deck\", substring(col(\"cabin\"), 1, 1))\n",
    "         .withColumn(\"ChildOrNot\", when(col(\"Age\")<18, \"Child\").otherwise(\"Adult\"))\n",
    "         .withColumn(\"MotherOrNot\", when((col(\"Age\")>18) & (col(\"Sex\")=='female') & (col(\"Parch\") > 0) & \n",
    "                                     (col(\"Title\") != 'Miss'), \"Mother\").otherwise(\"NotAMother\"))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(total.filter(\"Survived != 'null'\")\n",
    " .selectExpr(\"CAST(Survived as INT) AS Survived\", \"MotherOrNot\")\n",
    " .groupby(\"MotherOrNot\").mean(\"Survived\").show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Mothers have a high probability of Surviving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(total.filter(\"Survived != 'null'\")\n",
    " .selectExpr(\"CAST(Survived as INT) AS Survived\", \"ChildOrNot\")\n",
    " .groupby(\"ChildOrNot\").mean(\"Survived\").show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.toPandas().isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Filling Embarked Column</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.filter(\"Embarked IS NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(total.filter(\"Embarked IS NOT NULL\").groupBy([\"Pclass\", \"Embarked\"])\n",
    " .agg(expr(\"percentile_approx(Fare, 0.5)\").alias('median')).show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for Pclass = 1 and Fare =~ 80, the embarked is 'C', so for missing records we can replaec embarked with the value 'C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total.na.fill('C', subset = 'Embarked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Filling Fare Column </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.filter(\"Fare IS NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_fare = (total.groupBy([\"Pclass\", \"Embarked\"])\n",
    " .agg(expr(\"percentile_approx(Fare, 0.5)\").alias('MEDIAN'), expr(\"mean(Fare)\").alias(\"MEAN\"))\n",
    " .filter(\"Pclass = 3 AND Embarked = 'S'\")\n",
    " .selectExpr(\"ROUND((MEAN + MEDIAN)/2, 2)\")\n",
    ").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total.na.fill(missing_fare, subset = 'Fare')\n",
    "total.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Filling Cabin and Deck Columns with 'UNK' Value</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total.na.fill('UNK', subset = [\"Cabin\", \"Deck\"])\n",
    "total = total.na.fill(-1, subset = \"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using pandas for easy analysis and understandability\n",
    "total.toPandas().isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocessed = total.filter(\"Survived = 'null'\")\n",
    "train_preprocessed = total.filter(\"Survived != 'null'\").withColumn(\"label\", expr(\"CAST(Survived AS DOUBLE)\"))\n",
    "train_preprocessed.count(), test_preprocessed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Saving The preprocessed files </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting into pandas and saving the file into csv\n",
    "train_preprocessed.toPandas().reset_index(drop = True).to_csv(DATASET_PATH + \"train_preprocessed.csv\", index = False)\n",
    "test_preprocessed.toPandas().reset_index(drop = True).to_csv(DATASET_PATH + \"test_preprocessed.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Defining Categorical and Continuous Variables for Modelling </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \n",
    "                 \"Deck\", \"Embarked\", \"Title\", \"SurName\", \"FamilyGroup\", \n",
    "                 \"ChildOrNot\", \"MotherOrNot\"]\n",
    "continuous_columns = [\"Age\", \"Fare\"]\n",
    "categorical_columns = [column_name for column_name in train_columns if column_name not in continuous_columns]\n",
    "categorical_columns, continuous_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Defining StringIndexers, OneHotEncoders for Categorical Columns </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing StringIndexers, OneHotEncoders to include them in a pipeline\n",
    "string_indexer = []\n",
    "onehot_encoder = []\n",
    "for column_name in categorical_columns : \n",
    "    #Defining String Indexer\n",
    "    stringindexer = StringIndexer(inputCol = column_name, outputCol = column_name + \"_indexed\").fit(train_preprocessed).setHandleInvalid(\"keep\")\n",
    "    train_preprocessed = stringindexer.transform(train_preprocessed)\n",
    "    #Defining OneHotEncoder\n",
    "    onehotencoder = OneHotEncoder(inputCol = column_name + \"_indexed\", outputCol = column_name + \"_encoded\").fit(train_preprocessed).setHandleInvalid(\"keep\")\n",
    "    train_preprocessed = onehotencoder.transform(train_preprocessed)\n",
    "    #Transforming Test data\n",
    "    test_preprocessed = stringindexer.transform(test_preprocessed)\n",
    "    test_preprocessed = onehotencoder.transform(test_preprocessed)\n",
    "    string_indexer.append(stringindexer)\n",
    "    onehot_encoder.append(onehotencoder)\n",
    "string_indexer, onehot_encoder                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Categorical feature Name after preprocessing(StringIndexing + OneHotEncoding)\n",
    "transformed_categorical_features = [column_name + \"_encoded\" for column_name in categorical_columns]\n",
    "transformed_categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combinig All the columns\n",
    "train_columns = continuous_columns + transformed_categorical_features\n",
    "train_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Defining Vector Assembler to Wrap all the columns into a single Column and Standard Scaler to Scale the features </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_assembler = VectorAssembler(inputCols = train_columns, outputCol = \"features\")\n",
    "train_preprocessed = vector_assembler.transform(train_preprocessed)\n",
    "standard_scaler = StandardScaler(inputCol = \"features\", outputCol = \"scaled_features\").fit(train_preprocessed)\n",
    "train_preprocessed = standard_scaler.transform(train_preprocessed)\n",
    "test_preprocessed = vector_assembler.transform(test_preprocessed)\n",
    "test_preprocessed = standard_scaler.transform(test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_preprocessed.randomSplit([0.8, 0.2])\n",
    "train_data.count(), val_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Defining the Random Forest Calssifier with input, output features and other default parameters</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(featuresCol = \"scaled_features\", labelCol = \"label\")\n",
    "rf_model = rf_clf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = rf_model.transform(val_data)\n",
    "val_pred.select(\"Survived\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.transform(test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator=BinaryClassificationEvaluator(rawPredictionCol=\"label\",labelCol=\"prediction\")\n",
    "print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(val_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the stages for model pipeline\n",
    "stages = string_indexer + onehot_encoder + [vector_assembler, standard_scaler, rf_clf]\n",
    "model_pipeline = Pipeline(stages = stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = model_pipeline.fit(total.filter(\"Survived != 'null'\").withColumn(\"label\", expr(\"CAST(Survived AS DOUBLE)\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pipeline_model.transform(total.filter(\"Survived = 'null'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred.selectExpr(\"PassengerId\", \"CAST(prediction AS INT)\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = spark.read.csv(DATASET_PATH + 'gender_submission.csv', header = True, inferSchema = True)\n",
    "submission.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the preditions to submission file\n",
    "(test_pred.selectExpr(\"PassengerId\", \"CAST(prediction AS INT) AS Survived\")\n",
    " .repartition(1)\n",
    " .write.option(\"header\",\"true\")\n",
    " .format(\"csv\")\n",
    " .mode(\"overwrite\")\n",
    " .save(DATASET_PATH + 'pyspark_rf_submission')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
